{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess as subp\n",
    "import json\n",
    "import numpy as np\n",
    "from scipy import stats, integrate\n",
    "import scipy.io as sio\n",
    "import pandas as pd\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "import seaborn as sns; sns.set(color_codes=True)\n",
    "import matplotlib.pyplot as plt\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import nibabel as nib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rpy2.ipython extension is already loaded. To reload it, use:\n",
      "  %reload_ext rpy2.ipython\n",
      "The oct2py.ipython extension is already loaded. To reload it, use:\n",
      "  %reload_ext oct2py.ipython\n"
     ]
    }
   ],
   "source": [
    "# load good stuff\n",
    "%matplotlib inline\n",
    "%load_ext rpy2.ipython\n",
    "%load_ext oct2py.ipython\n",
    "sns.set(color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adding library hcp to the search path."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Adding library niak to the search path."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Adding library psom to the search path."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add niak hcp and psomm to oactve path\n",
    "%octave addpath(genpath('~/git/Misc'));\n",
    "%octave build_path hcp niak psom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom function for key sorting element\n",
    "import re\n",
    "\n",
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "def natural_keys(text):\n",
    "    '''\n",
    "    alist.sort(key=natural_keys) sorts in human order\n",
    "    http://nedbatchelder.com/blog/200712/human_sorting.html\n",
    "    (See Toothy's implementation in the comments)\n",
    "    '''\n",
    "    return [ atoi(c) for c in re.split('(\\d+)', text) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path \n",
    "#path_root = '/home/yassinebha/data/data_disk/Drive/HCP2/Solar_heritability/HCP_subtype/'\n",
    "path_root = '/scratch/yassinebha/pleio/pleio_association/'\n",
    "#  Set file pattern\n",
    "pedig_file_ptrn = 'solar_{}_spm_pedigre.csv'\n",
    "pheno_file_ptrn = 'solar_{}_spm_{}_pheno.csv'\n",
    "pheno_clust = os.path.join(path_root,'hcp_bootstraped_pheno_nonan_norm_python_13.csv')\n",
    "# Set task and trials\n",
    "task_name = 'wm'\n",
    "nb_sbt= 5\n",
    "list_sbt = ['sub%s' % str(i+1) for i in range(nb_sbt)]\n",
    "list_trial = ['2bk','0bk','contrast_2bk_vs_0bk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sub1', 'sub2', 'sub3', 'sub4', 'sub5']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_sbt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load pedig file\n",
    "pedig_df = pd.read_csv(os.path.join(path_root,pedig_file_ptrn.format(str.upper(task_name))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                             803\n",
       "WM_2bk_sub1                    803\n",
       "WM_2bk_sub2                    803\n",
       "WM_2bk_sub3                    803\n",
       "WM_2bk_sub4                    803\n",
       "WM_2bk_sub5                    803\n",
       "WM_contrast_2bk_vs_0bk_sub1    803\n",
       "WM_contrast_2bk_vs_0bk_sub2    803\n",
       "WM_contrast_2bk_vs_0bk_sub3    803\n",
       "WM_contrast_2bk_vs_0bk_sub4    803\n",
       "WM_contrast_2bk_vs_0bk_sub5    803\n",
       "WM_0bk_sub1                    803\n",
       "WM_0bk_sub2                    803\n",
       "WM_0bk_sub3                    803\n",
       "WM_0bk_sub4                    803\n",
       "WM_0bk_sub5                    803\n",
       "Age_in_Yrs                     803\n",
       "Gender                         803\n",
       "BMI                            803\n",
       "WM_FD_mean                     803\n",
       "WM_FD_scrubbed_mean            803\n",
       "dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load subtypes weights\n",
    "sbt_weight_df = pd.read_csv(os.path.join(path_root,'solar_{}_spm_{}_pheno.csv'.format(str.upper(task_name),\n",
    "                                                                                 str(nb_sbt))))\n",
    "sbt_weight_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Cluster_1</th>\n",
       "      <th>Cluster_2</th>\n",
       "      <th>Cluster_3</th>\n",
       "      <th>Cluster_4</th>\n",
       "      <th>Cluster_5</th>\n",
       "      <th>Cluster_6</th>\n",
       "      <th>Cluster_7</th>\n",
       "      <th>Cluster_8</th>\n",
       "      <th>Cluster_9</th>\n",
       "      <th>Cluster_10</th>\n",
       "      <th>Cluster_11</th>\n",
       "      <th>Cluster_12</th>\n",
       "      <th>Cluster_13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HCP100004</td>\n",
       "      <td>-1.051773</td>\n",
       "      <td>0.134739</td>\n",
       "      <td>-0.538317</td>\n",
       "      <td>1.185358</td>\n",
       "      <td>0.085162</td>\n",
       "      <td>-0.279920</td>\n",
       "      <td>0.466965</td>\n",
       "      <td>0.832836</td>\n",
       "      <td>-1.530547</td>\n",
       "      <td>-0.097661</td>\n",
       "      <td>0.398510</td>\n",
       "      <td>0.082329</td>\n",
       "      <td>-0.624942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HCP100206</td>\n",
       "      <td>-1.143145</td>\n",
       "      <td>-0.344595</td>\n",
       "      <td>1.301892</td>\n",
       "      <td>1.283574</td>\n",
       "      <td>1.436342</td>\n",
       "      <td>0.052662</td>\n",
       "      <td>0.706008</td>\n",
       "      <td>1.282970</td>\n",
       "      <td>-0.510366</td>\n",
       "      <td>0.205767</td>\n",
       "      <td>1.975280</td>\n",
       "      <td>-0.303875</td>\n",
       "      <td>0.064552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HCP100307</td>\n",
       "      <td>-0.528705</td>\n",
       "      <td>-0.639736</td>\n",
       "      <td>0.492726</td>\n",
       "      <td>0.522789</td>\n",
       "      <td>-0.713226</td>\n",
       "      <td>-0.952467</td>\n",
       "      <td>-0.559144</td>\n",
       "      <td>-0.166309</td>\n",
       "      <td>-0.140952</td>\n",
       "      <td>0.387172</td>\n",
       "      <td>-0.703027</td>\n",
       "      <td>-0.249928</td>\n",
       "      <td>-0.420702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HCP100408</td>\n",
       "      <td>-0.237051</td>\n",
       "      <td>1.173656</td>\n",
       "      <td>0.018709</td>\n",
       "      <td>0.092160</td>\n",
       "      <td>-0.034026</td>\n",
       "      <td>-0.521888</td>\n",
       "      <td>-0.109229</td>\n",
       "      <td>0.865643</td>\n",
       "      <td>0.221359</td>\n",
       "      <td>-0.082267</td>\n",
       "      <td>-0.589268</td>\n",
       "      <td>0.011946</td>\n",
       "      <td>0.191045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HCP100610</td>\n",
       "      <td>2.017489</td>\n",
       "      <td>0.891096</td>\n",
       "      <td>0.894110</td>\n",
       "      <td>-0.361182</td>\n",
       "      <td>-0.386394</td>\n",
       "      <td>-0.881930</td>\n",
       "      <td>-0.575076</td>\n",
       "      <td>0.206005</td>\n",
       "      <td>-0.197974</td>\n",
       "      <td>0.709730</td>\n",
       "      <td>0.203063</td>\n",
       "      <td>0.229882</td>\n",
       "      <td>1.150436</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID  Cluster_1  Cluster_2  Cluster_3  Cluster_4  Cluster_5  \\\n",
       "0  HCP100004  -1.051773   0.134739  -0.538317   1.185358   0.085162   \n",
       "1  HCP100206  -1.143145  -0.344595   1.301892   1.283574   1.436342   \n",
       "2  HCP100307  -0.528705  -0.639736   0.492726   0.522789  -0.713226   \n",
       "3  HCP100408  -0.237051   1.173656   0.018709   0.092160  -0.034026   \n",
       "4  HCP100610   2.017489   0.891096   0.894110  -0.361182  -0.386394   \n",
       "\n",
       "   Cluster_6  Cluster_7  Cluster_8  Cluster_9  Cluster_10  Cluster_11  \\\n",
       "0  -0.279920   0.466965   0.832836  -1.530547   -0.097661    0.398510   \n",
       "1   0.052662   0.706008   1.282970  -0.510366    0.205767    1.975280   \n",
       "2  -0.952467  -0.559144  -0.166309  -0.140952    0.387172   -0.703027   \n",
       "3  -0.521888  -0.109229   0.865643   0.221359   -0.082267   -0.589268   \n",
       "4  -0.881930  -0.575076   0.206005  -0.197974    0.709730    0.203063   \n",
       "\n",
       "   Cluster_12  Cluster_13  \n",
       "0    0.082329   -0.624942  \n",
       "1   -0.303875    0.064552  \n",
       "2   -0.249928   -0.420702  \n",
       "3    0.011946    0.191045  \n",
       "4    0.229882    1.150436  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load phenotype clustered file\n",
    "pheno_clust_df = pd.read_csv(pheno_clust).rename(columns={'Subject': 'ID'})\n",
    "pheno_clust_df.loc[:,'ID'] = 'HCP' + pheno_clust_df['ID'].astype(str)\n",
    "pheno_clust_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cluster_1',\n",
       " 'Cluster_2',\n",
       " 'Cluster_3',\n",
       " 'Cluster_4',\n",
       " 'Cluster_5',\n",
       " 'Cluster_6',\n",
       " 'Cluster_7',\n",
       " 'Cluster_8',\n",
       " 'Cluster_9',\n",
       " 'Cluster_10',\n",
       " 'Cluster_11',\n",
       " 'Cluster_12',\n",
       " 'Cluster_13']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clusters list\n",
    "list_pheno_clust = [x for x in pheno_clust_df.columns.get_values() if x!='ID']\n",
    "list_pheno_clust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                             803\n",
       "WM_2bk_sub1                    803\n",
       "WM_2bk_sub2                    803\n",
       "WM_2bk_sub3                    803\n",
       "WM_2bk_sub4                    803\n",
       "WM_2bk_sub5                    803\n",
       "WM_contrast_2bk_vs_0bk_sub1    803\n",
       "WM_contrast_2bk_vs_0bk_sub2    803\n",
       "WM_contrast_2bk_vs_0bk_sub3    803\n",
       "WM_contrast_2bk_vs_0bk_sub4    803\n",
       "WM_contrast_2bk_vs_0bk_sub5    803\n",
       "WM_0bk_sub1                    803\n",
       "WM_0bk_sub2                    803\n",
       "WM_0bk_sub3                    803\n",
       "WM_0bk_sub4                    803\n",
       "WM_0bk_sub5                    803\n",
       "Age_in_Yrs                     803\n",
       "Gender                         803\n",
       "BMI                            803\n",
       "WM_FD_mean                     803\n",
       "WM_FD_scrubbed_mean            803\n",
       "Subject                        803\n",
       "Cluster_1                      803\n",
       "Cluster_2                      803\n",
       "Cluster_3                      803\n",
       "Cluster_4                      803\n",
       "Cluster_5                      803\n",
       "Cluster_6                      803\n",
       "Cluster_7                      803\n",
       "Cluster_8                      803\n",
       "Cluster_9                      803\n",
       "Cluster_10                     803\n",
       "Cluster_11                     803\n",
       "Cluster_12                     803\n",
       "Cluster_13                     803\n",
       "dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge pheno clusters with subtypes\n",
    "sbt_pheno_df = pd.merge(sbt_weight_df,pheno_clust_df,how='left',left_on='ID',right_on='Subject')\n",
    "sbt_pheno_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if any NaN value present\n",
    "sbt_pheno_df.isnull().values.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pleiothropy estimate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import multiprocessing \n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2bk Cluster_1 sub1\n",
      "2bk Cluster_1 sub2\n",
      "2bk Cluster_1 sub3\n",
      "2bk Cluster_1 sub4\n",
      "2bk Cluster_1 sub5\n",
      "2bk Cluster_2 sub1\n",
      "2bk Cluster_2 sub2\n",
      "2bk Cluster_2 sub3\n",
      "2bk Cluster_2 sub4\n",
      "2bk Cluster_2 sub5\n",
      "2bk Cluster_3 sub1\n",
      "2bk Cluster_3 sub2\n",
      "2bk Cluster_3 sub3\n",
      "2bk Cluster_3 sub4\n",
      "2bk Cluster_3 sub5\n",
      "2bk Cluster_4 sub1\n",
      "2bk Cluster_4 sub2\n",
      "2bk Cluster_4 sub3\n",
      "2bk Cluster_4 sub4\n",
      "2bk Cluster_4 sub5\n",
      "2bk Cluster_5 sub1\n",
      "2bk Cluster_5 sub2\n",
      "2bk Cluster_5 sub3\n",
      "2bk Cluster_5 sub4\n",
      "2bk Cluster_5 sub5\n",
      "2bk Cluster_6 sub1\n",
      "2bk Cluster_6 sub2\n",
      "2bk Cluster_6 sub3\n",
      "2bk Cluster_6 sub4\n",
      "2bk Cluster_6 sub5\n",
      "2bk Cluster_7 sub1\n",
      "2bk Cluster_7 sub2\n",
      "2bk Cluster_7 sub3\n",
      "2bk Cluster_7 sub4\n",
      "2bk Cluster_7 sub5\n",
      "2bk Cluster_8 sub1\n",
      "2bk Cluster_8 sub2\n",
      "2bk Cluster_8 sub3\n",
      "2bk Cluster_8 sub4\n",
      "2bk Cluster_8 sub5\n",
      "2bk Cluster_9 sub1\n",
      "2bk Cluster_9 sub2\n",
      "2bk Cluster_9 sub3\n",
      "2bk Cluster_9 sub4\n",
      "2bk Cluster_9 sub5\n",
      "2bk Cluster_10 sub1\n",
      "2bk Cluster_10 sub2\n",
      "2bk Cluster_10 sub3\n",
      "2bk Cluster_10 sub4\n",
      "2bk Cluster_10 sub5\n",
      "2bk Cluster_11 sub1\n",
      "2bk Cluster_11 sub2\n",
      "2bk Cluster_11 sub3\n",
      "2bk Cluster_11 sub4\n",
      "2bk Cluster_11 sub5\n",
      "2bk Cluster_12 sub1\n",
      "2bk Cluster_12 sub2\n",
      "2bk Cluster_12 sub3\n",
      "2bk Cluster_12 sub4\n",
      "2bk Cluster_12 sub5\n",
      "2bk Cluster_13 sub1\n",
      "2bk Cluster_13 sub2\n",
      "2bk Cluster_13 sub3\n",
      "2bk Cluster_13 sub4\n",
      "2bk Cluster_13 sub5\n",
      "0bk Cluster_1 sub1\n",
      "0bk Cluster_1 sub2\n",
      "0bk Cluster_1 sub3\n",
      "0bk Cluster_1 sub4\n",
      "0bk Cluster_1 sub5\n",
      "0bk Cluster_2 sub1\n",
      "0bk Cluster_2 sub2\n",
      "0bk Cluster_2 sub3\n",
      "0bk Cluster_2 sub4\n",
      "0bk Cluster_2 sub5\n",
      "0bk Cluster_3 sub1\n",
      "0bk Cluster_3 sub2\n",
      "0bk Cluster_3 sub3\n",
      "0bk Cluster_3 sub4\n",
      "0bk Cluster_3 sub5\n",
      "0bk Cluster_4 sub1\n",
      "0bk Cluster_4 sub2\n",
      "0bk Cluster_4 sub3\n",
      "0bk Cluster_4 sub4\n",
      "0bk Cluster_4 sub5\n",
      "0bk Cluster_5 sub1\n",
      "0bk Cluster_5 sub2\n",
      "0bk Cluster_5 sub3\n",
      "0bk Cluster_5 sub4\n",
      "0bk Cluster_5 sub5\n",
      "0bk Cluster_6 sub1\n",
      "0bk Cluster_6 sub2\n",
      "0bk Cluster_6 sub3\n",
      "0bk Cluster_6 sub4\n",
      "0bk Cluster_6 sub5\n",
      "0bk Cluster_7 sub1\n",
      "0bk Cluster_7 sub2\n",
      "0bk Cluster_7 sub3\n",
      "0bk Cluster_7 sub4\n",
      "0bk Cluster_7 sub5\n",
      "0bk Cluster_8 sub1\n",
      "0bk Cluster_8 sub2\n",
      "0bk Cluster_8 sub3\n",
      "0bk Cluster_8 sub4\n",
      "0bk Cluster_8 sub5\n",
      "0bk Cluster_9 sub1\n",
      "0bk Cluster_9 sub2\n",
      "0bk Cluster_9 sub3\n",
      "0bk Cluster_9 sub4\n",
      "0bk Cluster_9 sub5\n",
      "0bk Cluster_10 sub1\n",
      "0bk Cluster_10 sub2\n",
      "0bk Cluster_10 sub3\n",
      "0bk Cluster_10 sub4\n",
      "0bk Cluster_10 sub5\n",
      "0bk Cluster_11 sub1\n",
      "0bk Cluster_11 sub2\n",
      "0bk Cluster_11 sub3\n",
      "0bk Cluster_11 sub4\n",
      "0bk Cluster_11 sub5\n",
      "0bk Cluster_12 sub1\n",
      "0bk Cluster_12 sub2\n",
      "0bk Cluster_12 sub3\n",
      "0bk Cluster_12 sub4\n",
      "0bk Cluster_12 sub5\n",
      "0bk Cluster_13 sub1\n",
      "0bk Cluster_13 sub2\n",
      "0bk Cluster_13 sub3\n",
      "0bk Cluster_13 sub4\n",
      "0bk Cluster_13 sub5\n",
      "contrast_2bk_vs_0bk Cluster_1 sub1\n",
      "contrast_2bk_vs_0bk Cluster_1 sub2\n",
      "contrast_2bk_vs_0bk Cluster_1 sub3\n",
      "contrast_2bk_vs_0bk Cluster_1 sub4\n",
      "contrast_2bk_vs_0bk Cluster_1 sub5\n",
      "contrast_2bk_vs_0bk Cluster_2 sub1\n",
      "contrast_2bk_vs_0bk Cluster_2 sub2\n",
      "contrast_2bk_vs_0bk Cluster_2 sub3\n",
      "contrast_2bk_vs_0bk Cluster_2 sub4\n",
      "contrast_2bk_vs_0bk Cluster_2 sub5\n",
      "contrast_2bk_vs_0bk Cluster_3 sub1\n",
      "contrast_2bk_vs_0bk Cluster_3 sub2\n",
      "contrast_2bk_vs_0bk Cluster_3 sub3\n",
      "contrast_2bk_vs_0bk Cluster_3 sub4\n",
      "contrast_2bk_vs_0bk Cluster_3 sub5\n",
      "contrast_2bk_vs_0bk Cluster_4 sub1\n",
      "contrast_2bk_vs_0bk Cluster_4 sub2\n",
      "contrast_2bk_vs_0bk Cluster_4 sub3\n",
      "contrast_2bk_vs_0bk Cluster_4 sub4\n",
      "contrast_2bk_vs_0bk Cluster_4 sub5\n",
      "contrast_2bk_vs_0bk Cluster_5 sub1\n",
      "contrast_2bk_vs_0bk Cluster_5 sub2\n",
      "contrast_2bk_vs_0bk Cluster_5 sub3\n",
      "contrast_2bk_vs_0bk Cluster_5 sub4\n",
      "contrast_2bk_vs_0bk Cluster_5 sub5\n",
      "contrast_2bk_vs_0bk Cluster_6 sub1\n",
      "contrast_2bk_vs_0bk Cluster_6 sub2\n",
      "contrast_2bk_vs_0bk Cluster_6 sub3\n",
      "contrast_2bk_vs_0bk Cluster_6 sub4\n",
      "contrast_2bk_vs_0bk Cluster_6 sub5\n",
      "contrast_2bk_vs_0bk Cluster_7 sub1\n",
      "contrast_2bk_vs_0bk Cluster_7 sub2\n",
      "contrast_2bk_vs_0bk Cluster_7 sub3\n",
      "contrast_2bk_vs_0bk Cluster_7 sub4\n",
      "contrast_2bk_vs_0bk Cluster_7 sub5\n",
      "contrast_2bk_vs_0bk Cluster_8 sub1\n",
      "contrast_2bk_vs_0bk Cluster_8 sub2\n",
      "contrast_2bk_vs_0bk Cluster_8 sub3\n",
      "contrast_2bk_vs_0bk Cluster_8 sub4\n",
      "contrast_2bk_vs_0bk Cluster_8 sub5\n",
      "contrast_2bk_vs_0bk Cluster_9 sub1\n",
      "contrast_2bk_vs_0bk Cluster_9 sub2\n",
      "contrast_2bk_vs_0bk Cluster_9 sub3\n",
      "contrast_2bk_vs_0bk Cluster_9 sub4\n",
      "contrast_2bk_vs_0bk Cluster_9 sub5\n",
      "contrast_2bk_vs_0bk Cluster_10 sub1\n",
      "contrast_2bk_vs_0bk Cluster_10 sub2\n",
      "contrast_2bk_vs_0bk Cluster_10 sub3\n",
      "contrast_2bk_vs_0bk Cluster_10 sub4\n",
      "contrast_2bk_vs_0bk Cluster_10 sub5\n",
      "contrast_2bk_vs_0bk Cluster_11 sub1\n",
      "contrast_2bk_vs_0bk Cluster_11 sub2\n",
      "contrast_2bk_vs_0bk Cluster_11 sub3\n",
      "contrast_2bk_vs_0bk Cluster_11 sub4\n",
      "contrast_2bk_vs_0bk Cluster_11 sub5\n",
      "contrast_2bk_vs_0bk Cluster_12 sub1\n",
      "contrast_2bk_vs_0bk Cluster_12 sub2\n",
      "contrast_2bk_vs_0bk Cluster_12 sub3\n",
      "contrast_2bk_vs_0bk Cluster_12 sub4\n",
      "contrast_2bk_vs_0bk Cluster_12 sub5\n",
      "contrast_2bk_vs_0bk Cluster_13 sub1\n",
      "contrast_2bk_vs_0bk Cluster_13 sub2\n",
      "contrast_2bk_vs_0bk Cluster_13 sub3\n",
      "contrast_2bk_vs_0bk Cluster_13 sub4\n",
      "contrast_2bk_vs_0bk Cluster_13 sub5\n"
     ]
    }
   ],
   "source": [
    "#initiate empty dictionary\n",
    "Pleio_Asso = collections.namedtuple('Pleio_Asso', [\n",
    "    'out_dir',\n",
    "    'pedig_f',\n",
    "    'pheno_f',\n",
    "    'var_1',\n",
    "    'var_2',\n",
    "    'covar_1'\n",
    "])\n",
    "pleio_asso = []\n",
    "\n",
    "# pleio root folder\n",
    "path_pleio = os.path.join(path_root,'pleio_{}_{}'.format(str(datetime.date.today()),\n",
    "                                                        task_name))\n",
    "if not os.path.exists(path_pleio):\n",
    "    os.makedirs(path_pleio)\n",
    "\n",
    "# create ouput folders and populate RhoG dictionnary\n",
    "for ix_trial, trial in enumerate(list_trial):\n",
    "    for ix_clust, clust in enumerate(list_pheno_clust):\n",
    "        for ix_subt, subt in enumerate(list_sbt):\n",
    "            print(trial,clust,subt)\n",
    "            pheno_1 = clust\n",
    "            pheno_2 = '{}_{}_{}'.format(str.upper(task_name),trial,subt)\n",
    "            cov_1 = '{}_FD_scrubbed_mean'.format(str.upper(task_name))\n",
    "        \n",
    "            # output result folder \n",
    "            path_pleio_contrast = os.path.join(path_pleio,'{}_{}'.format(trial,subt))\n",
    "            if not os.path.exists(path_pleio_contrast):\n",
    "                os.makedirs(path_pleio_contrast)\n",
    "            \n",
    "            # copy needed files to output folder\n",
    "            if not os.path.isfile(os.path.join(path_pleio_contrast,'se_univ_polygen.tcl')):\n",
    "                subp.run(['cp',os.path.join(path_root,'se_univ_polygen.tcl'),path_pleio_contrast])\n",
    "            if not os.path.isfile(os.path.join(path_pleio_contrast,'pleio_run.sh')):\n",
    "                subp.run(['cp',os.path.join(path_root,'pleio_run.sh'),path_pleio_contrast])\n",
    "        \n",
    "        # pedegree\n",
    "        if not os.path.isfile(os.path.join(path_pleio_contrast,pedig_file_ptrn.format(str.upper(task_name)))):\n",
    "            subp.run(['cp',os.path.join(path_root,pedig_file_ptrn.format(str.upper(task_name))),path_pleio_contrast]) \n",
    "        \n",
    "        pedig_f = os.path.join(path_pleio_contrast,pedig_file_ptrn.format(str.upper(task_name)))\n",
    "        \n",
    "        # save pheno cov to file\n",
    "        pheno_f= os.path.join(path_pleio_contrast,'pheno_cov.csv')\n",
    "        if not os.path.isfile(pheno_f):\n",
    "            sbt_pheno_df[['ID','Age_in_Yrs','Gender',pheno_1,pheno_2,cov_1]].to_csv(pheno_f,index=False)   \n",
    "        \n",
    "        # collect all pleio contrasts in dictionary\n",
    "        pleio_asso.append(Pleio_Asso(out_dir = path_pleio_contrast,\n",
    "                                      pedig_f = pedig_f,\n",
    "                                      pheno_f = pheno_f,\n",
    "                                      var_1 = pheno_1,\n",
    "                                      var_2 = pheno_2,\n",
    "                                      covar_1 = cov_1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pleio(x):\n",
    "    subp.run(['bash', os.path.join(x.out_dir,'pleio_run.sh'),\n",
    "              x.out_dir,\n",
    "              x.pedig_f,\n",
    "              x.pheno_f,\n",
    "              x.var_1,\n",
    "              x.var_2,\n",
    "              x.covar_1,\n",
    "             ])\n",
    "    \n",
    "    # collect result\n",
    "    RhoE = ''\n",
    "    RhoE_pval=''\n",
    "    RhoG = ''\n",
    "    RhoG_pval_0 = ''\n",
    "    RhoG_pval_1 = ''\n",
    "    results = ''\n",
    "    \n",
    "    contrast_name = '{}_{}'.format(x.var_1,x.var_2)\n",
    "    fp = open(os.path.join(x.out_dir,'solar_pleio.out'))\n",
    "    for i,line in enumerate(fp):\n",
    "        if 'CONVERGENCE FAILURE' in line:\n",
    "            print('{}_{}'.format(x.var1,x.var_2))\n",
    "            print(line)\n",
    "            results = {'contrast_name' : contrast_name,'no_converg' : True}\n",
    "            break\n",
    "        if 'RhoE is ' in line:\n",
    "            RhoE = float(line.strip('\\n').split(' ')[3])\n",
    "            RhoE_pval = float(line.strip('\\n').split(' ')[-1])\n",
    "        if 'RhoG is ' in line:\n",
    "            RhoG = float(line.strip('\\n').split(' ')[-1])\n",
    "            #print(out_dir)\n",
    "            #print(line.strip('\\t ').strip('\\n'))\n",
    "        if 'RhoG different from zero' in line:\n",
    "            RhoG_pval_0 = float(line.strip('\\n').split(' ')[-1])\n",
    "            #print(line.strip('\\t '))\n",
    "        if 'RhoG different from -1.0' in line:\n",
    "            RhoG_pval_1 = float(line.strip('\\n').split(' ')[-1])\n",
    "            results = {'contrast_name' : contrast_name,'no_converg' : False,\n",
    "                       'RhoE' : RhoE,\n",
    "                       'RhoE_pval' : RhoE_pval,\n",
    "                       'RhoG' : RhoG,\n",
    "                       'RhoG_pval_0' : RhoG_pval_0,\n",
    "                       'RhoG_pval_1' : RhoG_pval_1\n",
    "                      }\n",
    "            break\n",
    "        elif 'RhoG different from 1.0' in line:\n",
    "            RohG_pval_1 = float(line.strip('\\n').split(' ')[-1])\n",
    "            results = {'contrast_name' : contrast_name,'no_converg' : False,\n",
    "                       'RhoE' : RhoE,\n",
    "                       'RhoE_pval' : RhoE_pval,\n",
    "                       'RhoG' : RhoG,\n",
    "                       'RhoG_pval_0' : RhoG_pval_0,\n",
    "                       'RhoG_pval_1' : RhoG_pval_1\n",
    "                      }\n",
    "            break\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function from http://danshiebler.com/2016-09-14-parallel-progress-bar/\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "def parallel_process(array, function, n_jobs=12, use_kwargs=False, front_num=3):\n",
    "    \"\"\"\n",
    "        A parallel version of the map function with a progress bar. \n",
    "\n",
    "        Args:\n",
    "            array (array-like): An array to iterate over.\n",
    "            function (function): A python function to apply to the elements of array\n",
    "            n_jobs (int, default=16): The number of cores to use\n",
    "            use_kwargs (boolean, default=False): Whether to consider the elements of array as dictionaries of \n",
    "                keyword arguments to function \n",
    "            front_num (int, default=3): The number of iterations to run serially before kicking off the parallel job. \n",
    "                Useful for catching bugs\n",
    "        Returns:\n",
    "            [function(array[0]), function(array[1]), ...]\n",
    "    \"\"\"\n",
    "    #We run the first few iterations serially to catch bugs\n",
    "    if front_num > 0:\n",
    "        front = [function(**a) if use_kwargs else function(a) for a in array[:front_num]]\n",
    "    #If we set n_jobs to 1, just run a list comprehension. This is useful for benchmarking and debugging.\n",
    "    if n_jobs==1:\n",
    "        return front + [function(**a) if use_kwargs else function(a) for a in tqdm(array[front_num:])]\n",
    "    #Assemble the workers\n",
    "    with ProcessPoolExecutor(max_workers=n_jobs) as pool:\n",
    "        #Pass the elements of array into function\n",
    "        if use_kwargs:\n",
    "            futures = [pool.submit(function, **a) for a in array[front_num:]]\n",
    "        else:\n",
    "            futures = [pool.submit(function, a) for a in array[front_num:]]\n",
    "        kwargs = {\n",
    "            'total': len(futures),\n",
    "            'unit': 'it',\n",
    "            'unit_scale': True,\n",
    "            'leave': True\n",
    "        }\n",
    "        #Print out the progress as tasks complete\n",
    "        for f in tqdm(as_completed(futures), **kwargs):\n",
    "            pass\n",
    "    out = []\n",
    "    #Get the results from the futures. \n",
    "    for i, future in tqdm(enumerate(futures)):\n",
    "        try:\n",
    "            out.append(future.result())\n",
    "        except Exception as e:\n",
    "            out.append(e)\n",
    "    return front + out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['bash', '/scratch/yassinebha/pleio/pleio_association/pleio_2018-08-06_wm/2bk_sub5/pleio_run.sh', '/scratch/yassinebha/pleio/pleio_association/pleio_2018-08-06_wm/2bk_sub5', '/scratch/yassinebha/pleio/pleio_association/pleio_2018-08-06_wm/2bk_sub5/solar_WM_spm_pedigre.csv', '/scratch/yassinebha/pleio/pleio_association/pleio_2018-08-06_wm/2bk_sub5/pheno_cov.csv', 'Cluster_1', 'WM_2bk_sub5'], returncode=0)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run one job for debuging\n",
    "subp.run(['bash', os.path.join(pleio_asso[0].out_dir,'pleio_run.sh'),\n",
    "              pleio_asso[0].out_dir,\n",
    "              pleio_asso[0].pedig_f,\n",
    "              pleio_asso[0].pheno_f,\n",
    "              pleio_asso[0].var_1,\n",
    "              pleio_asso[0].var_2\n",
    "         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36.0/36.0 [00:00<00:00, 89.8it/s]\n",
      "36it [00:00, 198677.56it/s]\n"
     ]
    }
   ],
   "source": [
    "Results = parallel_process(pleio_asso,run_pleio,use_kwargs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix_trial, trial in enumerate(list_trial):\n",
    "    for ix_clust, clust in enumerate(list_pheno_clust):\n",
    "        for ix_subt, subt in enumerate(list_sbt):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect results\n",
    "RohG= np.eye(len(pheno_clean_df.columns.drop('ID')))\n",
    "RohG_pval_0 = np.eye(len(pheno_clean_df.columns.drop('ID')))\n",
    "RohG_pval_1 = np.eye(len(pheno_clean_df.columns.drop('ID')))\n",
    "RohE= np.eye(len(pheno_clean_df.columns.drop('ID')))\n",
    "RohE_pval = np.eye(len(pheno_clean_df.columns.drop('ID')))\n",
    "RohP= np.eye(len(pheno_clean_df.columns.drop('ID')))\n",
    "RohP_pval = np.eye(len(pheno_clean_df.columns.drop('ID')))\n",
    "count = 0\n",
    "No_converge = collections.namedtuple('No_converge', [\n",
    "    'var_1',\n",
    "    'var_2',\n",
    "    'pedig_f',\n",
    "    'pheno_f',\n",
    "    'out_dir'\n",
    "])\n",
    "no_converges = []\n",
    "pedig_f = os.path.join(path_root,'pleio_all_pheno_pedig.csv')\n",
    "for ix_pheno_1, pheno_1 in enumerate(pheno_clean_df.columns.drop('ID')):\n",
    "    for ix_pheno_2, pheno_2 in enumerate(pheno_clean_df.columns.drop('ID')):\n",
    "        if pheno_1  == pheno_2:\n",
    "            continue\n",
    "        out_dir = '{}_{}'.format(pheno_1,pheno_2)\n",
    "        path_pleio_contrast = os.path.join(path_pleio,'{}_{}'.format(pheno_1,pheno_2))\n",
    "        pedig_f = os.path.join(path_pleio_contrast,'pleio_all_pheno_pedig.csv')\n",
    "        pheno_f= os.path.join(path_pleio_contrast,'pheno_cov.csv')\n",
    "        \n",
    "        #number_lines = sum(1 for line in open(os.path.join(path_root,out_dir,'solar_pleio.out')))\n",
    "        fp = open(os.path.join(path_pleio,out_dir,'solar_pleio.out'))\n",
    "        for i,line in enumerate(fp):\n",
    "            if 'CONVERGENCE FAILURE' in line:\n",
    "                print(out_dir)\n",
    "                print(line)\n",
    "                count+=1\n",
    "                no_converges.append(No_converge(var_1 = pheno_1,\n",
    "                                                var_2 = pheno_2,\n",
    "                                                out_dir = out_dir,\n",
    "                                                pedig_f = pedig_f,\n",
    "                                                pheno_f = pedig_f))\n",
    "                break\n",
    "                \n",
    "            if 'RhoE is ' in line:\n",
    "                RohE[ix_pheno_1,ix_pheno_2] = float(line.strip('\\n').split(' ')[3])\n",
    "                RohE_pval[ix_pheno_1,ix_pheno_2] = float(line.strip('\\n').split(' ')[-1])\n",
    "                \n",
    "            if 'RhoG is ' in line:\n",
    "                RohG[ix_pheno_1,ix_pheno_2] = float(line.strip('\\n').split(' ')[-1])\n",
    "                \n",
    "                #print(out_dir)\n",
    "                #print(line.strip('\\t ').strip('\\n'))\n",
    "            if 'RhoG different from zero' in line:\n",
    "                RohG_pval_0[ix_pheno_1,ix_pheno_2] = float(line.strip('\\n').split(' ')[-1])\n",
    "                \n",
    "                #print(line.strip('\\t '))\n",
    "            if 'RhoG different from -1.0' in line:\n",
    "                RohG_pval_1[ix_pheno_1,ix_pheno_2] = float(line.strip('\\n').split(' ')[-1])\n",
    "                \n",
    "            elif 'RhoG different from 1.0' in line:\n",
    "                RohG_pval_1[ix_pheno_1,ix_pheno_2] = float(line.strip('\\n').split(' ')[-1])\n",
    "                \n",
    "            if 'RhoP is ' in line:\n",
    "                #print(float(line.strip('\\n').split(' ')[-1]))\n",
    "                RohP[ix_pheno_1,ix_pheno_2] = float(line.strip('\\n').split(' ')[-1])\n",
    "                \n",
    "            if 'RhoP different from zero' in line:\n",
    "                RohP_pval[ix_pheno_1,ix_pheno_2] = float(line.strip('\\n').split(' ')[-1])\n",
    "                #print(float(line.strip('\\n').split(' ')[-1]))\n",
    "                break           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and merge all weight and pheno data\n",
    "dict_all_weight_pheno = {}\n",
    "dict_all_weight_pheno_clust = {}\n",
    "dict_all_weight_pheno_clust_json = {}\n",
    "\n",
    "for ind_f , task_folder in enumerate(list_subtype_folder):\n",
    "    \n",
    "    # set path and task name\n",
    "    path_subtype = os.path.join(path_root,task_folder);\n",
    "    path_association =  os.path.join(path_subtype,'associations/');\n",
    "    path_networks =  os.path.join(path_subtype,'networks/');\n",
    "    # number of subtype\n",
    "    nb_sbt = natural_keys(path_subtype)[9]\n",
    "    # task name\n",
    "    task_name = natural_keys(path_subtype)[10][5:-1]\n",
    "    # List phenotypes\n",
    "    list_pheno  = [f for f in os.listdir(path_association)]\n",
    "    list_pheno.sort(key=natural_keys)\n",
    "    # List trials\n",
    "    list_trial  = [f for f in os.listdir(path_networks)]\n",
    "    # List subtype\n",
    "    list_subtype = ['sub{}'.format(ii) for ii in range(1,nb_sbt+1)]\n",
    "    \n",
    "    # collect weight data\n",
    "    for ind_t, trial_name in enumerate(list_trial) :\n",
    "        # collect all pheno data\n",
    "        all_pheno_clust = []\n",
    "        names_pheno = []\n",
    "        for ix, pheno_name in enumerate(list_pheno):\n",
    "            #from IPython.core.debugger import Tracer; Tracer()() \n",
    "            mat_file = os.path.join(path_association,pheno_name,'association_stats_{}.mat'.format(pheno_name))\n",
    "            %octave_push mat_file\n",
    "            %octave mat_load = load(mat_file);\n",
    "            %octave_pull mat_load\n",
    "            model_x_norm = mat_load['model_norm']['x']\n",
    "            model_labels_y_norm = mat_load['model_norm']['labels_y']\n",
    "            model_labels_x_norm = mat_load['model_norm']['labels_x']\n",
    "\n",
    "            my_pheno = np.array(model_x_norm[:,1])\n",
    "            my_pheno_name= model_labels_y_norm[1]\n",
    "            all_pheno_clust.append(my_pheno)\n",
    "            names_pheno.append(my_pheno_name)\n",
    "\n",
    "        # create pheno dataframe\n",
    "        all_pheno_clust = np.concatenate([model_labels_x_norm[...,None],np.transpose(all_pheno_clust)],axis=1)\n",
    "        all_pheno_clust_df = pd.DataFrame(all_pheno_clust,columns=np.append('ID',  list_pheno))\n",
    "        all_pheno_clust_df['ID'] = all_pheno_clust_df.ID.str.strip()\n",
    "\n",
    "        #collect weight and create dataframe\n",
    "        weight_file = os.path.join(path_networks,'{}/sbt_weights_net_{}.csv'.format(trial_name,trial_name))\n",
    "        weight_df = pd.read_csv(weight_file)\n",
    "        column_names =np.append('ID',  list_subtype)\n",
    "        weight_df.columns= column_names\n",
    "        weight_df['ID'] = weight_df.ID.str.strip()\n",
    "\n",
    "        # merge weight amd pheno dataframe\n",
    "        weight_pheno_clust_df=pd.merge(weight_df,all_pheno_clust_df,on='ID',how='left')\n",
    "        # save it csv\n",
    "        weight_pheno_clust_df.to_csv(os.path.join(path_subtype,'{}_weight_pheno_subtype.csv'.format(trial_name)))\n",
    "\n",
    "        # stack to dictionary\n",
    "        dict_all_weight_pheno_clust[task_name + \"_\" + \n",
    "                                    trial_name + \"_\" + \n",
    "                                    str(nb_sbt) + '_subtypes'] = weight_pheno_clust_df\n",
    "        # stack to json to be saved later\n",
    "        dict_all_weight_pheno_clust_json[task_name + \"_\" +\n",
    "                                         trial_name + \"_\" +\n",
    "                                         str(nb_sbt) + '_subtypes'] = weight_pheno_clust_df.to_json(orient='split')\n",
    "        # Merge all pheno with subtype weights\n",
    "        all_pheno_pruned = pd.merge(weight_pheno_clust_df[['ID']+list_subtype],all_pheno,on='ID',how='left')\n",
    "        # Drop NaN\n",
    "        all_pheno_pruned.dropna(inplace=True)\n",
    "        dict_all_weight_pheno[task_name + \"_\" +\n",
    "                              trial_name + \"_\" +\n",
    "                              str(nb_sbt) + '_subtypes'] = all_pheno_pruned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
